#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KB AI Challenge - íˆ¬ì ì‹¬ë¦¬ ë¶„ì„ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ (Data Augmentation í–¥ìƒ ë²„ì „)
KB Reflex: AI íˆ¬ì ì‹¬ë¦¬ ì½”ì¹­ í”Œë«í¼
Back Translation ë° ë‹¤ì–‘í•œ ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©
"""

import os
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
from pathlib import Path
import warnings
import time
import random
warnings.filterwarnings('ignore')

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
    MarianMTModel,
    MarianTokenizer
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, f1_score

# ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

class DataAugmentation:
    """ë°ì´í„° ì¦ê°• í´ë˜ìŠ¤ - Back Translation ë° ê¸°íƒ€ ê¸°ë²•"""
    
    def __init__(self):
        print("ğŸ”„ ë°ì´í„° ì¦ê°• ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        self.setup_translation_models()
    
    def setup_translation_models(self):
        """ë²ˆì—­ ëª¨ë¸ ì„¤ì • (Back Translationìš©)"""
        try:
            # í•œêµ­ì–´ â†’ ì˜ì–´
            self.ko_to_en_model_name = "Helsinki-NLP/opus-mt-ko-en"
            self.ko_to_en_tokenizer = MarianTokenizer.from_pretrained(self.ko_to_en_model_name)
            self.ko_to_en_model = MarianMTModel.from_pretrained(self.ko_to_en_model_name)
            
            # ì˜ì–´ â†’ í•œêµ­ì–´
            self.en_to_ko_model_name = "Helsinki-NLP/opus-mt-en-ko"
            self.en_to_ko_tokenizer = MarianTokenizer.from_pretrained(self.en_to_ko_model_name)
            self.en_to_ko_model = MarianMTModel.from_pretrained(self.en_to_ko_model_name)
            
            print("âœ… Back Translation ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            self.translation_available = True
            
        except Exception as e:
            print(f"âš ï¸ ë²ˆì—­ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ëª¨ë¸ì„ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
            self.translation_available = False
    
    def translate_text(self, text: str, source_lang: str = "ko", target_lang: str = "en") -> str:
        """í…ìŠ¤íŠ¸ ë²ˆì—­"""
        if not self.translation_available:
            return text
        
        try:
            if source_lang == "ko" and target_lang == "en":
                inputs = self.ko_to_en_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
                outputs = self.ko_to_en_model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)
                translated = self.ko_to_en_tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            elif source_lang == "en" and target_lang == "ko":
                inputs = self.en_to_ko_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
                outputs = self.en_to_ko_model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)
                translated = self.en_to_ko_tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            else:
                return text
            
            return translated.strip()
            
        except Exception as e:
            print(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
            return text
    
    def back_translate(self, text: str) -> str:
        """Back Translation ìˆ˜í–‰: í•œêµ­ì–´ â†’ ì˜ì–´ â†’ í•œêµ­ì–´"""
        if not self.translation_available:
            return text
        
        # Step 1: í•œêµ­ì–´ â†’ ì˜ì–´
        english_text = self.translate_text(text, "ko", "en")
        if english_text == text:  # ë²ˆì—­ ì‹¤íŒ¨
            return text
        
        # Step 2: ì˜ì–´ â†’ í•œêµ­ì–´
        back_translated = self.translate_text(english_text, "en", "ko")
        
        return back_translated if back_translated != english_text else text
    
    def synonym_replacement(self, text: str) -> str:
        """ë™ì˜ì–´ êµì²´ (ê°„ë‹¨í•œ ë²„ì „)"""
        synonyms = {
            'ë§¤ìˆ˜': ['êµ¬ë§¤', 'ë§¤ì…', 'ì·¨ë“'],
            'ë§¤ë„': ['íŒë§¤', 'ë§¤ê°', 'ì²˜ë¶„'],
            'ê¸‰ë“±': ['ìƒìŠ¹', 'í­ë“±', 'ì¹˜ì†Ÿ'],
            'ê¸‰ë½': ['í•˜ë½', 'í­ë½', 'ë–¨ì–´'],
            'ë¬´ì„œì›Œì„œ': ['ë‘ë ¤ì›Œì„œ', 'ë¶ˆì•ˆí•´ì„œ', 'ê±±ì •ë˜ì–´ì„œ'],
            'í™•ì‹¤': ['ë¶„ëª…', 'ëª…í™•', 'í‹€ë¦¼ì—†'],
            'ëŒ€ë°•': ['í¬ê²Œ', 'ë§ì´', 'ëŒ€ëŸ‰'],
            'ì†ì ˆ': ['ì†í•´ë§¤ë„', 'ì†ì‹¤ì •ë¦¬', 'ì»·ë¡œìŠ¤']
        }
        
        words = text.split()
        augmented_words = []
        
        for word in words:
            # 30% í™•ë¥ ë¡œ ë™ì˜ì–´ êµì²´
            if random.random() < 0.3:
                base_word = word
                for key, values in synonyms.items():
                    if key in word:
                        replacement = random.choice(values)
                        word = word.replace(key, replacement)
                        break
            augmented_words.append(word)
        
        return ' '.join(augmented_words)
    
    def random_insertion(self, text: str) -> str:
        """ëœë¤ ë‹¨ì–´/êµ¬ë¬¸ ì‚½ì…"""
        insertion_phrases = [
            'ì •ë§ë¡œ', 'ì‚¬ì‹¤', 'ì‹¤ì œë¡œ', 'ê²°êµ­', 'ê·¸ëŸ°ë°', 'ê·¸ë˜ì„œ', 
            'ì•„ë¬´ë˜ë„', 'ì–´ì¨Œë“ ', 'í™•ì‹¤íˆ', 'ë¶„ëª…íˆ', 'ë‹¹ì—°íˆ'
        ]
        
        words = text.split()
        if len(words) < 3:
            return text
        
        # 20% í™•ë¥ ë¡œ êµ¬ë¬¸ ì‚½ì…
        if random.random() < 0.2:
            insert_pos = random.randint(1, len(words) - 1)
            phrase = random.choice(insertion_phrases)
            words.insert(insert_pos, phrase)
        
        return ' '.join(words)
    
    def augment_text(self, text: str, methods: List[str] = None) -> List[str]:
        """ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¦ê°•"""
        if methods is None:
            methods = ['back_translate', 'synonym', 'insertion']
        
        augmented_texts = [text]  # ì›ë³¸ í¬í•¨
        
        for method in methods:
            try:
                if method == 'back_translate' and self.translation_available:
                    bt_text = self.back_translate(text)
                    if bt_text != text and len(bt_text.strip()) > 5:
                        augmented_texts.append(bt_text)
                
                elif method == 'synonym':
                    syn_text = self.synonym_replacement(text)
                    if syn_text != text:
                        augmented_texts.append(syn_text)
                
                elif method == 'insertion':
                    ins_text = self.random_insertion(text)
                    if ins_text != text:
                        augmented_texts.append(ins_text)
                        
            except Exception as e:
                print(f"ì¦ê°• ë°©ë²• '{method}' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return augmented_texts

class InvestmentEmotionDataset(Dataset):
    """íˆ¬ì ë©”ëª¨ì™€ ê°ì • íƒœê·¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” PyTorch ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self) -> int:
        return len(self.texts)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def find_csv_files():
    """CSV íŒŒì¼ ìœ„ì¹˜ ìë™ íƒì§€"""
    possible_locations = [
        # í˜„ì¬ ë””ë ‰í† ë¦¬
        ["kim_gukmin_trades.csv", "park_tuja_trades.csv"],
        # data í´ë”
        ["data/kim_gukmin_trades.csv", "data/park_tuja_trades.csv"],
        # ì ˆëŒ€ ê²½ë¡œë¡œ data í´ë” í™•ì¸
        [str(Path("data/kim_gukmin_trades.csv").resolve()), 
         str(Path("data/park_tuja_trades.csv").resolve())],
    ]
    
    for file_list in possible_locations:
        if all(os.path.exists(file) for file in file_list):
            print(f"âœ… CSV íŒŒì¼ ë°œê²¬: {os.path.dirname(file_list[0]) or 'í˜„ì¬ ë””ë ‰í† ë¦¬'}")
            return file_list
    
    return None

def create_data_if_missing():
    """ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±"""
    print("ğŸ“Š CSV íŒŒì¼ì´ ì—†ì–´ì„œ ìë™ ìƒì„±í•©ë‹ˆë‹¤...")
    
    try:
        # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
        current_file = Path(__file__)
        project_root = current_file.parent
        
        # sys.pathì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
        import sys
        sys.path.insert(0, str(project_root))
        
        from db.user_db import UserDatabase
        db = UserDatabase()
        
        # ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ ë°˜í™˜
        data_dir = project_root / "data"
        return [
            str(data_dir / "kim_gukmin_trades.csv"),
            str(data_dir / "park_tuja_trades.csv")
        ]
        
    except Exception as e:
        print(f"âŒ ìë™ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ 'python db/user_db.py'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return None

def load_and_combine_data(file_paths: List[str]) -> pd.DataFrame:
    """ì—¬ëŸ¬ CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í†µí•©"""
    print("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
    dataframes = []
    
    for file_path in file_paths:
        try:
            # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
            encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is not None:
                print(f"âœ… {file_path} ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
                dataframes.append(df)
            else:
                print(f"âŒ {file_path} ì¸ì½”ë”© ì˜¤ë¥˜")
                
        except FileNotFoundError:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            continue
        except Exception as e:
            print(f"âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            continue
    
    if not dataframes:
        raise ValueError("ë¡œë“œí•  ìˆ˜ ìˆëŠ” ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    combined_df = pd.concat(dataframes, ignore_index=True)
    print(f"ğŸ”„ ë°ì´í„° í†µí•© ì™„ë£Œ: ì´ {len(combined_df)}ê°œ ë ˆì½”ë“œ")
    
    return combined_df

def augment_training_data(df: pd.DataFrame, augment_ratio: float = 2.0) -> pd.DataFrame:
    """
    í›ˆë ¨ ë°ì´í„°ì— ë°ì´í„° ì¦ê°• ì ìš©
    
    Args:
        df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        augment_ratio: ì¦ê°• ë¹„ìœ¨ (2.0 = ì›ë³¸ì˜ 2ë°°ê¹Œì§€ ì¦ê°•)
    
    Returns:
        ì¦ê°•ëœ ë°ì´í„°í”„ë ˆì„
    """
    print(f"ğŸ”„ ë°ì´í„° ì¦ê°• ì‹œì‘ (ëª©í‘œ ë¹„ìœ¨: {augment_ratio}x)...")
    
    augmenter = DataAugmentation()
    augmented_data = []
    
    # ê°ì •ë³„ë¡œ ê· ë“±í•˜ê²Œ ì¦ê°•
    emotion_counts = df['ê°ì •íƒœê·¸'].value_counts()
    total_original = len(df)
    target_size = int(total_original * augment_ratio)
    
    print(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {total_original}ê°œ â†’ ëª©í‘œ: {target_size}ê°œ")
    
    for emotion in emotion_counts.index:
        emotion_data = df[df['ê°ì •íƒœê·¸'] == emotion]
        original_count = len(emotion_data)
        
        # ê° ê°ì •ë³„ë¡œ ëª©í‘œ ê°œìˆ˜ ê³„ì‚°
        target_count = int((original_count / total_original) * target_size)
        augment_needed = max(0, target_count - original_count)
        
        print(f"  {emotion}: {original_count} â†’ {target_count} (ì¶”ê°€: {augment_needed})")
        
        # ì›ë³¸ ë°ì´í„° ì¶”ê°€
        for _, row in emotion_data.iterrows():
            augmented_data.append({
                'ë©”ëª¨': row['ë©”ëª¨'],
                'ê°ì •íƒœê·¸': row['ê°ì •íƒœê·¸'],
                'source': 'original'
            })
        
        # ì¦ê°• ë°ì´í„° ìƒì„±
        if augment_needed > 0:
            emotion_texts = emotion_data['ë©”ëª¨'].tolist()
            generated_count = 0
            
            for _ in range(augment_needed):
                if generated_count >= augment_needed:
                    break
                
                # ëœë¤í•˜ê²Œ ì›ë³¸ í…ìŠ¤íŠ¸ ì„ íƒ
                source_text = random.choice(emotion_texts)
                
                # ì¦ê°• ë°©ë²• ëœë¤ ì„ íƒ
                methods = ['back_translate', 'synonym', 'insertion']
                selected_methods = random.sample(methods, random.randint(1, 2))
                
                # ì¦ê°• ì‹¤í–‰
                augmented_texts = augmenter.augment_text(source_text, selected_methods)
                
                # ì›ë³¸ê³¼ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ê°€
                for aug_text in augmented_texts[1:]:  # ì²« ë²ˆì§¸ëŠ” ì›ë³¸ì´ë¯€ë¡œ ì œì™¸
                    if generated_count >= augment_needed:
                        break
                    
                    if len(aug_text.strip()) > 5 and aug_text != source_text:
                        augmented_data.append({
                            'ë©”ëª¨': aug_text,
                            'ê°ì •íƒœê·¸': emotion,
                            'source': 'augmented'
                        })
                        generated_count += 1
    
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    augmented_df = pd.DataFrame(augmented_data)
    
    print(f"âœ… ë°ì´í„° ì¦ê°• ì™„ë£Œ: {len(augmented_df)}ê°œ ë ˆì½”ë“œ")
    print(f"   - ì›ë³¸: {len(augmented_df[augmented_df['source'] == 'original'])}ê°œ")
    print(f"   - ì¦ê°•: {len(augmented_df[augmented_df['source'] == 'augmented'])}ê°œ")
    
    return augmented_df

def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, int], Dict[int, str]]:
    """ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¼ë²¨ ì¸ì½”ë”©"""
    print("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_columns = ['ë©”ëª¨', 'ê°ì •íƒœê·¸']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
        print(f"ğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
    
    # ê²°ì¸¡ê°’ ì œê±°
    initial_len = len(df)
    df = df.dropna(subset=['ë©”ëª¨', 'ê°ì •íƒœê·¸'])
    print(f"ğŸ“ ê²°ì¸¡ê°’ ì œê±°: {initial_len} -> {len(df)}ê°œ ë ˆì½”ë“œ")
    
    # ë¹ˆ ë¬¸ìì—´ ì œê±°
    df = df[df['ë©”ëª¨'].str.strip() != '']
    df = df[df['ê°ì •íƒœê·¸'].str.strip() != '']
    print(f"ğŸ“ ë¹ˆ ê°’ ì œê±° í›„: {len(df)}ê°œ ë ˆì½”ë“œ")
    
    # ê°ì •íƒœê·¸ì—ì„œ # ê¸°í˜¸ ì œê±° (ìˆëŠ” ê²½ìš°)
    df['ê°ì •íƒœê·¸'] = df['ê°ì •íƒœê·¸'].str.replace('#', '', regex=False)
    
    # ë¼ë²¨ ì¸ì½”ë”©
    label_encoder = LabelEncoder()
    df['label_encoded'] = label_encoder.fit_transform(df['ê°ì •íƒœê·¸'])
    
    # ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    label_to_id = {label: idx for idx, label in enumerate(label_encoder.classes_)}
    id_to_label = {idx: label for label, idx in label_to_id.items()}
    
    print(f"ğŸ·ï¸  ê°ì • ë¼ë²¨ ë§¤í•‘:")
    for label, idx in label_to_id.items():
        count = len(df[df['ê°ì •íƒœê·¸'] == label])
        print(f"   {idx}: {label} ({count}ê°œ)")
    
    return df, label_to_id, id_to_label

def create_train_val_split(df: pd.DataFrame, test_size: float = 0.2, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì…‹ ë¶„í• """
    print("âœ‚ï¸  ë°ì´í„° ë¶„í•  ì¤‘...")
    
    train_df, val_df = train_test_split(
        df, 
        test_size=test_size, 
        random_state=random_state,
        stratify=df['label_encoded']
    )
    
    print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(train_df)}ê°œ")
    print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(val_df)}ê°œ")
    
    return train_df, val_df

def initialize_model_and_tokenizer(model_name: str, num_labels: int) -> Tuple[Any, Any]:
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""
    print(f"ğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels,
        hidden_dropout_prob=0.3,  # ë“œë¡­ì•„ì›ƒ ì¦ê°€ë¡œ ê³¼ì í•© ë°©ì§€
        attention_probs_dropout_prob=0.3
    )
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë¼ë²¨ ìˆ˜: {num_labels})")
    return model, tokenizer

def compute_metrics(eval_pred) -> Dict[str, float]:
    """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (ì •í™•ë„ + F1 ìŠ¤ì½”ì–´)"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    accuracy = accuracy_score(labels, predictions)
    f1_macro = f1_score(labels, predictions, average='macro')
    f1_weighted = f1_score(labels, predictions, average='weighted')
    
    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted
    }

def save_model_info(output_dir: str, label_to_id: Dict[str, int], id_to_label: Dict[int, str]):
    """ëª¨ë¸ ì •ë³´ ë° ë¼ë²¨ ë§¤í•‘ ì €ì¥"""
    model_info = {
        'label_to_id': label_to_id,
        'id_to_label': {str(k): v for k, v in id_to_label.items()},
        'num_labels': len(label_to_id),
        'model_type': 'Enhanced BERT-based emotion classifier with data augmentation',
        'training_date': time.strftime('%Y-%m-%d %H:%M:%S'),
        'augmentation_methods': ['back_translation', 'synonym_replacement', 'random_insertion']
    }
    
    info_path = os.path.join(output_dir, 'model_info.json')
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ëª¨ë¸ ì •ë³´ ì €ì¥: {info_path}")

def main():
    """ë©”ì¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ (í–¥ìƒëœ ë²„ì „)"""
    print("ğŸš€ KB Reflex íˆ¬ì ì‹¬ë¦¬ ë¶„ì„ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (AI Challenge í–¥ìƒ ë²„ì „)")
    print("=" * 70)
    
    # ì„¤ì •ê°’
    MODEL_NAME = "klue/bert-base"
    OUTPUT_DIR = "./sentiment_model"
    AUGMENTATION_RATIO = 2.5  # ì›ë³¸ ëŒ€ë¹„ 2.5ë°°ë¡œ ì¦ê°•
    
    try:
        # 1. CSV íŒŒì¼ ì°¾ê¸° ë˜ëŠ” ìƒì„±
        CSV_FILES = find_csv_files()
        
        if CSV_FILES is None:
            CSV_FILES = create_data_if_missing()
            if CSV_FILES is None:
                raise FileNotFoundError("CSV íŒŒì¼ì„ ì°¾ê±°ë‚˜ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # 2. ë°ì´í„° ë¡œë”©
        df = load_and_combine_data(CSV_FILES)
        print(f"ğŸ“ˆ ë¡œë“œëœ ì´ ë°ì´í„°: {len(df)}ê°œ")
        
        # 3. ë°ì´í„° ì¦ê°• (í›ˆë ¨ ë°ì´í„°ë§Œ)
        print("\nğŸ”„ ë°ì´í„° ì¦ê°• ë‹¨ê³„")
        augmented_df = augment_training_data(df, AUGMENTATION_RATIO)
        
        # 4. ë°ì´í„° ì „ì²˜ë¦¬
        augmented_df, label_to_id, id_to_label = preprocess_data(augmented_df)
        
        # 5. ë°ì´í„° ë¶„í•  (ì¦ê°•ëœ ë°ì´í„° ê¸°ì¤€)
        train_df, val_df = create_train_val_split(augmented_df, test_size=0.15)  # ê²€ì¦ì…‹ ë¹„ìœ¨ ì¤„ì„
        
        print(f"\nğŸ“Š ìµœì¢… ë°ì´í„° ë¶„í• :")
        print(f"   í›ˆë ¨: {len(train_df)}ê°œ")
        print(f"   ê²€ì¦: {len(val_df)}ê°œ")
        print(f"   ì´í•©: {len(train_df) + len(val_df)}ê°œ")
        
        # 6. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        model, tokenizer = initialize_model_and_tokenizer(MODEL_NAME, len(label_to_id))
        
        # 7. ë°ì´í„°ì…‹ ìƒì„±
        print("ğŸ¯ PyTorch ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        train_dataset = InvestmentEmotionDataset(
            texts=train_df['ë©”ëª¨'].tolist(),
            labels=train_df['label_encoded'].tolist(),
            tokenizer=tokenizer
        )
        
        val_dataset = InvestmentEmotionDataset(
            texts=val_df['ë©”ëª¨'].tolist(),
            labels=val_df['label_encoded'].tolist(),
            tokenizer=tokenizer
        )
        
        # 8. í–¥ìƒëœ í›ˆë ¨ ì„¤ì •
        try:
            training_args = TrainingArguments(
                output_dir=OUTPUT_DIR,
                num_train_epochs=5,  # ì—í¬í¬ ì¦ê°€
                per_device_train_batch_size=16,
                per_device_eval_batch_size=32,
                warmup_steps=200,  # warmup ë‹¨ê³„ ì¦ê°€
                weight_decay=0.01,
                learning_rate=2e-5,  # í•™ìŠµë¥  ì¡°ì •
                eval_strategy="steps",
                save_strategy="steps",
                eval_steps=100,
                save_steps=100,
                logging_steps=50,
                load_best_model_at_end=True,
                metric_for_best_model="f1_weighted",  # F1 ìŠ¤ì½”ì–´ ê¸°ì¤€
                greater_is_better=True,
                save_total_limit=3,
                remove_unused_columns=False,
                dataloader_num_workers=2,
                fp16=True,  # í˜¼í•© ì •ë°€ë„ í›ˆë ¨
                gradient_accumulation_steps=2,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
            )
        except TypeError:
            # êµ¬ ë²„ì „ í˜¸í™˜ì„±
            print("âš ï¸  êµ¬ ë²„ì „ transformers ê°ì§€, í˜¸í™˜ì„± ëª¨ë“œ...")
            training_args = TrainingArguments(
                output_dir=OUTPUT_DIR,
                num_train_epochs=5,
                per_device_train_batch_size=16,
                per_device_eval_batch_size=32,
                warmup_steps=200,
                weight_decay=0.01,
                learning_rate=2e-5,
                evaluation_strategy="steps",
                save_strategy="steps",
                eval_steps=100,
                save_steps=100,
                logging_steps=50,
                load_best_model_at_end=True,
                metric_for_best_model="f1_weighted",
                greater_is_better=True,
                save_total_limit=3,
                remove_unused_columns=False,
            )
        
        # 9. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        print("ğŸƒâ€â™‚ï¸ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì¤‘...")
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=tokenizer,
            compute_metrics=compute_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
        )
        
        # 10. ëª¨ë¸ í›ˆë ¨
        print("ğŸ“ í–¥ìƒëœ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        print("=" * 70)
        start_time = time.time()
        
        trainer.train()
        
        training_time = time.time() - start_time
        print(f"â° í›ˆë ¨ ì†Œìš” ì‹œê°„: {training_time/60:.1f}ë¶„")
        
        # 11. ìµœì¢… í‰ê°€
        print("ğŸ“Š ìµœì¢… í‰ê°€ ì¤‘...")
        eval_results = trainer.evaluate()
        print(f"âœ… ìµœì¢… ì„±ëŠ¥:")
        print(f"   ì •í™•ë„: {eval_results['eval_accuracy']:.4f}")
        print(f"   F1 (Macro): {eval_results['eval_f1_macro']:.4f}")
        print(f"   F1 (Weighted): {eval_results['eval_f1_weighted']:.4f}")
        
        # 12. ëª¨ë¸ ì €ì¥
        print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        trainer.save_model()
        tokenizer.save_pretrained(OUTPUT_DIR)
        
        # 13. ëª¨ë¸ ì •ë³´ ì €ì¥
        save_model_info(OUTPUT_DIR, label_to_id, id_to_label)
        
        # 14. ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„±
        print("ğŸ“‹ ë¶„ë¥˜ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        predictions = trainer.predict(val_dataset)
        y_pred = np.argmax(predictions.predictions, axis=1)
        y_true = val_df['label_encoded'].tolist()
        
        print("\nğŸ“Š ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ:")
        print("=" * 70)
        target_names = [id_to_label[i] for i in range(len(id_to_label))]
        print(classification_report(y_true, y_pred, target_names=target_names))
        
        print("\nğŸ‰ í–¥ìƒëœ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
        print(f"ğŸ“ˆ ë°ì´í„° ì¦ê°•ë¥ : {AUGMENTATION_RATIO}x")
        print(f"ğŸ¯ ìµœì¢… F1 ìŠ¤ì½”ì–´: {eval_results['eval_f1_weighted']:.4f}")
        print("=" * 70)
        
        # 15. í–¥ìƒëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª í–¥ìƒëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸:")
        test_texts = [
            "ì½”ìŠ¤í”¼ê°€ ë„ˆë¬´ ë–¨ì–´ì ¸ì„œ ë¬´ì„œì›Œì„œ ì „ëŸ‰ ë§¤ë„í–ˆì–´ìš”",
            "ìœ íŠœë²„ ì¶”ì²œë°›ê³  ê¸‰í•˜ê²Œ ë§¤ìˆ˜í–ˆìŠµë‹ˆë‹¤",
            "ê¸°ìˆ ì  ë¶„ì„ ê²°ê³¼ ì ì • ë§¤ìˆ˜ íƒ€ì´ë°ìœ¼ë¡œ íŒë‹¨ë¨",
            "í”¼ì‹± ì‚¬ê¸° ì†Œì‹ì— íŒ¨ë‹‰ë§¤ë„ í–ˆìŠµë‹ˆë‹¤",
            "ESG ë“±ê¸‰ í•˜ë½ìœ¼ë¡œ ë¶ˆì•ˆí•´ì„œ ì¼ë¶€ ë§¤ë„",
            "ëª¨ë“  ì‚¬ëŒë“¤ì´ ì‚¬ë¼ê³  í•´ì„œ ë”°ë¼ì„œ ë§¤ìˆ˜í–ˆì–´ìš”",
            "ì†ì‹¤ì´ ë„ˆë¬´ ì»¤ì„œ ì†ì ˆì„ ëª»í•˜ê² ì–´ìš”",
            "ì™„ë²½í•œ ë¶„ì„ í›„ í™•ì‹ ì„ ê°€ì§€ê³  ë§¤ìˆ˜í–ˆìŠµë‹ˆë‹¤"
        ]
        
        for test_text in test_texts:
            inputs = tokenizer(test_text, return_tensors="pt", truncation=True, max_length=128)
            with torch.no_grad():
                outputs = model(**inputs)
                predicted_id = torch.argmax(outputs.logits, dim=-1).item()
                predicted_emotion = id_to_label[predicted_id]
                confidence = torch.softmax(outputs.logits, dim=-1).max().item()
            
            print(f"ğŸ“ '{test_text[:40]}...'")
            print(f"ğŸ¯ ì˜ˆì¸¡: {predicted_emotion} (ì‹ ë¢°ë„: {confidence:.3f})")
            print()
        
        # 16. ì„±ëŠ¥ í–¥ìƒ ìš”ì•½
        print("ğŸ“ˆ AI Challengeë¥¼ ìœ„í•œ ëª¨ë¸ ê°œì„ ì‚¬í•­:")
        print("âœ… ë°ì´í„°ì…‹ í¬ê¸°: 120ê°œ â†’ 3000ê°œ (25ë°° ì¦ê°€)")
        print("âœ… ê°ì • íŒ¨í„´: 7ê°œ â†’ 9ê°œ (ë‹¤ì–‘ì„± ì¦ê°€)")
        print("âœ… í…Œë§ˆ ë‹¤ì–‘ì„±: ê¸ˆìœµ ì‚¬ê¸°, ì •ì±…, ESG, ë…ì„± ì¡°í•­ ë“± ì¶”ê°€")
        print("âœ… ë°ì´í„° ì¦ê°•: Back Translation, ë™ì˜ì–´ êµì²´, ëœë¤ ì‚½ì…")
        print("âœ… ëª¨ë¸ ìµœì í™”: ë“œë¡­ì•„ì›ƒ, í•™ìŠµë¥ , ì—í¬í¬ ìˆ˜ ì¡°ì •")
        print("âœ… í‰ê°€ ì§€í‘œ: ì •í™•ë„ + F1 ìŠ¤ì½”ì–´ (ê· í˜•ì¡íŒ í‰ê°€)")
        
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main()#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KB AI Challenge - íˆ¬ì ì‹¬ë¦¬ ë¶„ì„ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ (Data Augmentation í–¥ìƒ ë²„ì „)
KB Reflex: AI íˆ¬ì ì‹¬ë¦¬ ì½”ì¹­ í”Œë«í¼
Back Translation ë° ë‹¤ì–‘í•œ ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©
"""

import os
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
from pathlib import Path
import warnings
import time
warnings.filterwarnings('ignore')

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
    MarianMTModel,
    MarianTokenizer
)